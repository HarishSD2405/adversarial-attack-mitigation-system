# adversarial-attack-mitigation-framework

A framework for Adversarial Attack Detection and Mitigation for image classification tasks. 

## Training

The notebooks in the [`training`](training/) directory are meant to be run on Kaggle, where pickled model files are found after training for integration with the pipeline.

After running the models, the trained models' pickle files can be found in the "/kaggle/working" directory.

## Testing

Save the pickled models in the [`models`](models/) directory, downloaded after training the models.

It is recommended to create a separate *venv* or *Conda* environment to use for training (if done locally) and testing to avoid dependency conflicts.

Testing the models can be done using the respective notebooks in the [`tests`](tests/) directory. 

## Architecture

The proposed architecture encompasses a pipeline of models that analyze and process the image in an attempt to correctly classify adversarial and clean images.

![alt text](public/architecture_revision_0.png)

### Models

- Baseline Classification Model
    - Architecture: A simple CNN consisting of a few convolutional layers to extract features, followed by pooling layers to downsample, and finally, fully connected layers to perform classification.
    - Dataset: Standard, clean MNIST dataset.
    - Training Method: Standard supervised learning, to minimize cross-entropy loss. This model is not shown adversarial examples, making it highly susceptible to attacks.
- Detection Model
    - Architecture: A simple CNN, enough to distinguish between clean and adversarial images. The final layer will have 2 outpu neurons, one for each class.
    - Dataset: Custom dataset, including data from the original MNIST dataset and then adversarial images generated from the clean dataset using PGD (Projected Gradient Descent). The dataset must contain an equal number of clean and adversarial images. The labels determine whether a given image is "clean" or "adversarial"
    - Training Method: Standard supervised learning with a binary classifier. A binary cross-entropy loss function is used with the dataset to help learn the subtle differences between clean and adversarial images. It simply performs a classification on the nature of the image's security.
- Purification Model
    - Model Architecture: A denoising autoencoder with 2 parts, an encoder compressing the image and a decoder reconstructing it. The model will learn to reconstruct a clean iamge from a corrupted image.
    - Dataset: A custom dataset of paired images. The model is trained on pairs of clean and corrupted images. The corrupted images are generated by applying attacks like FGSM and PGD to the original dataset. The corresponsing clean images serve as the ground truth.
    - Training Method: Supervised learning with reconstruction loss. The model is trained to minimize the difference between output and the original clean image. The loss function used is MSE (Mean Squared Error), which measures pixel-by-pixel difference between 2 images. 
- Mitigation Model
    - Model Architecture: A CNN or a ResNet, with the latter being a potentially better choice due to its deeper architecture being better at handling complex data.
    - Dataset: The MNIST standard dataset, but the training phase dynamically augements it.
    - Training Method: Adversarial training, where adversarial examples are generated on the fly in real-time during the training loop. For each batch of clean images, PGD and FGSM is performed to create an adversarial example. The model is then trained on the combined batch of both the clean and adversarial iages sharing the common output labels. This tries to force the model to correctly classify both, clean and adversarial inputs, making it potentially more robust then the base model. 

## To-Do

- [ ] - Try out ResNet for the mitigation model
- [ ] - Also simulate FGSM attacks during the training of the mitigation model