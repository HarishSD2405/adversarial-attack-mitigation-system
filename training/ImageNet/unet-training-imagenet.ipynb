{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2491748,"sourceType":"datasetVersion","datasetId":1500837},{"sourceId":715609,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":543914,"modelId":557035}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install adversarial-robustness-toolbox","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-19T15:55:33.624271Z","iopub.execute_input":"2026-01-19T15:55:33.624895Z","iopub.status.idle":"2026-01-19T15:55:38.843286Z","shell.execute_reply.started":"2026-01-19T15:55:33.624856Z","shell.execute_reply":"2026-01-19T15:55:38.842429Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\nimport numpy as np\nfrom art.attacks.evasion import ProjectedGradientDescent\nfrom art.estimators.classification import PyTorchClassifier\n\n# =========================================================\n# Config\n# =========================================================\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nBATCH_SIZE = 32\nEPOCHS = 10\nLR = 1e-4\n\nIMG_SIZE = 224\nNUM_CLASSES = 100   # ImageNet-100\n\n# Loss weights\nLAMBDA_RECON   = 1.0\nLAMBDA_FEAT    = 0.5\nLAMBDA_CLS     = 0.2\nLAMBDA_RESID   = 0.05   # residual magnitude penalty\n\nCLEAN_PROB = 0.7        # identity preservation\n\nprint(\"Using device:\", DEVICE)\n\n# =========================================================\n# ImageNet normalization\n# =========================================================\nIMAGENET_MEAN = torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1).to(DEVICE)\nIMAGENET_STD  = torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1).to(DEVICE)\n\ndef normalize_imagenet(x):\n    return (x - IMAGENET_MEAN) / IMAGENET_STD\n\n# =========================================================\n# Dataset\n# =========================================================\ntransform_train = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(IMG_SIZE),\n    transforms.ToTensor()\n])\n\ntrain_dataset = torchvision.datasets.ImageFolder(\n    root=\"/kaggle/input/imagenet100\",\n    transform=transform_train\n)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True\n)\n\n# =========================================================\n# Frozen Classifier\n# =========================================================\nclassifier = torchvision.models.resnet18(weights=\"IMAGENET1K_V1\")\nclassifier.fc = nn.Linear(512, NUM_CLASSES)\nclassifier = classifier.to(DEVICE)\nclassifier.eval()\n\nfor p in classifier.parameters():\n    p.requires_grad = False\n\n# =========================================================\n# ART PGD (attack only)\n# =========================================================\ncriterion = nn.CrossEntropyLoss()\n\nart_classifier = PyTorchClassifier(\n    model=classifier,\n    loss=criterion,\n    optimizer=None,\n    input_shape=(3, IMG_SIZE, IMG_SIZE),\n    nb_classes=NUM_CLASSES,\n    device_type=DEVICE.type\n)\n\npgd_attack = ProjectedGradientDescent(\n    estimator=art_classifier,\n    eps=8/255,\n    eps_step=2/255,\n    max_iter=10,\n    batch_size=BATCH_SIZE\n)\n\n# =========================================================\n# UNet Purifier (Artifact-free, Residual)\n# =========================================================\nclass PurifierUNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        def block(in_c, out_c):\n            return nn.Sequential(\n                nn.Conv2d(in_c, out_c, 3, padding=1),\n                nn.GroupNorm(8, out_c),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(out_c, out_c, 3, padding=1),\n                nn.GroupNorm(8, out_c),\n                nn.ReLU(inplace=True)\n            )\n\n        def up_block(in_c, out_c):\n            return nn.Sequential(\n                nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),\n                nn.Conv2d(in_c, out_c, 3, padding=1),\n                nn.GroupNorm(8, out_c),\n                nn.ReLU(inplace=True)\n            )\n\n        self.enc1 = block(3, 64)\n        self.enc2 = block(64, 128)\n        self.enc3 = block(128, 256)\n\n        self.pool = nn.MaxPool2d(2)\n\n        self.bottleneck = block(256, 512)\n\n        self.up3 = up_block(512, 256)\n        self.dec3 = block(512, 256)\n\n        self.up2 = up_block(256, 128)\n        self.dec2 = block(256, 128)\n\n        self.up1 = up_block(128, 64)\n        self.dec1 = nn.Sequential(\n            nn.Conv2d(128, 64, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 3, 3, padding=1)  # residual output\n        )\n\n    def forward(self, x):\n        e1 = self.enc1(x)\n        e2 = self.enc2(self.pool(e1))\n        e3 = self.enc3(self.pool(e2))\n\n        b = self.bottleneck(self.pool(e3))\n\n        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1))\n        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n\n        return d1  # residual\n\npurifier = PurifierUNet().to(DEVICE)\n\n# =========================================================\n# Optimizer & Losses\n# =========================================================\noptimizer = optim.Adam(purifier.parameters(), lr=LR)\nrecon_loss = nn.L1Loss()\ncls_loss = nn.CrossEntropyLoss()\n\n# =========================================================\n# Multi-layer perceptual features (ResNet)\n# =========================================================\nfeat_l1 = nn.Sequential(\n    classifier.conv1,\n    classifier.bn1,\n    classifier.relu,\n    classifier.maxpool,\n    classifier.layer1\n).eval()\n\nfeat_l3 = nn.Sequential(\n    classifier.conv1,\n    classifier.bn1,\n    classifier.relu,\n    classifier.maxpool,\n    classifier.layer1,\n    classifier.layer2,\n    classifier.layer3\n).eval()\n\n# =========================================================\n# Training Loop\n# =========================================================\nfor epoch in range(EPOCHS):\n    purifier.train()\n    total_loss = 0.0\n\n    for x_clean, y in train_loader:\n        x_clean = x_clean.to(DEVICE)\n        y = y.to(DEVICE)\n\n        # --------------------\n        # Generate adversarial\n        # --------------------\n        x_adv = pgd_attack.generate(x_clean.cpu().numpy())\n        x_adv = torch.tensor(x_adv, device=DEVICE)\n\n        # --------------------\n        # Mix clean & adv\n        # --------------------\n        mask = (torch.rand(x_clean.size(0), 1, 1, 1, device=DEVICE) < CLEAN_PROB)\n        x_in = torch.where(mask, x_clean, x_adv)\n\n        # --------------------\n        # Residual purification\n        # --------------------\n        residual = purifier(x_in)\n        x_pur = torch.clamp(x_in + residual, 0, 1)\n\n        # --------------------\n        # Losses\n        # --------------------\n        loss_recon = recon_loss(x_pur, x_clean)\n\n        # Perceptual\n        with torch.no_grad():\n            f1_clean = feat_l1(normalize_imagenet(x_clean))\n            f3_clean = feat_l3(normalize_imagenet(x_clean))\n\n        f1_pur = feat_l1(normalize_imagenet(x_pur))\n        f3_pur = feat_l3(normalize_imagenet(x_pur))\n\n        loss_feat = (\n            recon_loss(f1_pur, f1_clean) +\n            0.5 * recon_loss(f3_pur, f3_clean)\n        )\n\n        # Classification (frozen)\n        with torch.no_grad():\n            logits = classifier(normalize_imagenet(x_pur))\n        loss_cls = cls_loss(logits, y)\n\n        # Residual magnitude regularization\n        loss_resid = torch.mean(torch.abs(residual))\n\n        # Total\n        loss = (\n            LAMBDA_RECON * loss_recon +\n            LAMBDA_FEAT  * loss_feat +\n            LAMBDA_CLS   * loss_cls +\n            LAMBDA_RESID * loss_resid\n        )\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f\"[Epoch {epoch+1}/{EPOCHS}] Loss: {total_loss / len(train_loader):.4f}\")\n\n# =========================================================\n# Save\n# =========================================================\ntorch.save(purifier.state_dict(), \"purifier_unet_imagenet_corrected.pth\")\nprint(\"Corrected ImageNet purifier saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-19T16:08:47.317750Z","iopub.execute_input":"2026-01-19T16:08:47.318402Z"}},"outputs":[],"execution_count":null}]}